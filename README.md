# QLearningデモプログラム
使用言語はpythonです。
コードは一部の修正を除きGemini 3.0Proにほとんど書かせましたが、内容は理解しています。

## 挙動について
(基本的な話については省略します)
移動可能なマスは4近傍(上下左右で、斜めはダメ)です。
このデモではε-greedyと呼ばれる手法を採用しています。
エージェントは一定確率εで「探索」と呼ばれる行動を、1-εの確率で「搾取」と呼ばれる行動を取ります。
内部的にQ値テーブルと呼ばれる二次元配列を持っており、各マスごとに、「そのマスに行くことで将来的に得られる報酬の期待値」が書かれています。
このQ値テーブルを試行により更新していくことで行動を改善します。初期値はゴールのマスが200、それ以外のマスが0です。

### 探索(exploration)
- ランダムに周囲に動いてみて、報酬が多くもらえるならその方向に移動する価値が高いと判断し、Q値テーブルを更新する。
- これがないとQ値テーブルが更新されないため学習もまったくできないが、この行動をとる回数が多すぎると学習が遅くなってしまう。

### 搾取(exploitation)
- 今いる場所から動けるマスのうち、最もQ値が高いマスへ移動します。
- 得られる報酬が高いとわかっている一方で、新しいルートを開拓することはありません。

強化学習はこの二つのバランスが大事で、うまく割合を調整することが強化学習の精度や学習速度改善に直結します。
デモプログラムのデフォルト値はε=0.1です。

### エージェント内に定義された定数α,γは？
#### α:学習率
今回得た経験をQ値テーブルにどれくらい反映させるか、という指標です。
学習率が高ければ一回の経験をすぐ学習し、低ければすぐには行動を変えず慎重に動きます。
高すぎると目先の行動のよさにはまってしまい、学習が進まなくなる可能性があります。
デフォルト値はα=0.1です。
#### γ:時間割引率
将来もらえる報酬を現時点でどれだけ重要視するか、という数値です。
時間割引率が高ければ数手先の行動の影響が大きく、低ければ直近の行動の影響しか見ることができません。
高ければ未来を見通せることになりますが、高すぎると計算に時間がかかったり、数手長いルートでも気にせず進んでしまうようになったりします。
デフォルト値はγ=0.9です。

## 「n回学習」ってどういうこと？
「エピソード(1回の試行)」をn回終えたことを表します。
- ゴールにたどり着くか一定回数の行動を終了するとエピソードは終了します。
- このプログラムではマスの数*2の値である1250回を1エピソードの行動上限回数として設定しています。
- エピソード内での行動によりQ値テーブルが書き換えられ、それにより次のエピソードに学習内容が引き継がれます。

このプログラムで生成されるヒートマップにおいて、n回学習後のマップは、n回のエピソードを終えたのちの100回、つまりn+1回目のエピソードからn+100回目のエピソードまでで各マスを通った回数を可視化します。
